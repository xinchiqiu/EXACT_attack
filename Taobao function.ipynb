{
  "metadata": {
    "dataExplorerConfig": {},
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "bento_kernel_pytorch",
      "cinder_runtime": false,
      "ipyflow_runtime": false,
      "metadata": {
        "kernel_name": "bento_kernel_pytorch",
        "nightly_builds": true,
        "fbpkg_supported": true,
        "cinder_runtime": false,
        "ipyflow_runtime": false,
        "is_prebuilt": true
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "last_server_session_id": "d4a930d5-a2e5-466e-9859-be24dc5f8c2b",
    "last_kernel_id": "9be6459a-4d88-4925-a955-5b455bc730ca",
    "last_base_url": "https://bento.edge.x2p.facebook.net/",
    "last_msg_id": "e38653db-84c4630d4850d071d42c4194_55",
    "outputWidgetContext": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "8b412db4-7357-4abf-8d1d-d07fc3fff76c",
        "showInput": true,
        "customInput": null
      },
      "source": [
        "\n",
        " # Dataset:\n",
        "\n",
        "## user related features\n",
        "cms_segid: 0-96\n",
        "\n",
        "cms_group_id: 0-12\n",
        "\n",
        "age: 0-6\n",
        "\n",
        "pvalue: 1,2,3\n",
        "\n",
        "shopping_level:1,2,3\n",
        "\n",
        "occupation: 1,0\n",
        "\n",
        "## from ad (onsite features)\n",
        "cate_id: 6725\n",
        "\n",
        "adgroup_id: 827009\n",
        "\n",
        "brand:98772\n",
        "\n",
        "customer:252841\n",
        "\n",
        "price: float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "cdefe75f-519a-48ce-b01d-baf53f9db294",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "1a9e5f3a-301b-45e7-9f5d-b7d14502c39a",
        "executionStartTime": 1677231309946,
        "executionStopTime": 1677231310021
      },
      "source": [
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_profiling \n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from iopath.common.file_io import PathManager\t\n",
        "from iopath.fb.manifold import ManifoldPathHandler\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import train_test_split\t\n",
        "import itertools\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "29405211-ca46-4461-b73c-88874cc4e846",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "37e495cf-3d5c-400d-b0bd-05f8ab7faf9b",
        "executionStartTime": 1677231310283,
        "executionStopTime": 1677231310367
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    #torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "    print(\"USING CUDA\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"USING CPU\")\n",
        "\n",
        "# sending model to cuda has error (not sure why)\n",
        "#device = 'cpu'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING CUDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7179b938-ab4c-4a23-9b7d-9045627c40bc",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "d3925bfa-934e-41c6-8e34-cbf90c410366",
        "executionStartTime": 1677231310766,
        "executionStopTime": 1677231310840
      },
      "source": [
        "def load_taobao_df():\n",
        "    path_manager = PathManager()\n",
        "    path_manager.register_handler(ManifoldPathHandler(has_user_data=False))\n",
        "    ad = \"manifold://fsl/tree/taobao/ad_feature.csv\"\n",
        "    ad_path = path_manager.open(ad, \"rb\")\n",
        "    ad_feature_df =  pd.read_csv(ad_path)\n",
        "\n",
        "    raw_sample = \"manifold://fsl/tree/taobao/raw_sample.csv\"\n",
        "    raw_sample_path = path_manager.open(raw_sample, \"rb\")\n",
        "    raw_sample_df = pd.read_csv(raw_sample_path)\n",
        "\n",
        "    user = \"manifold://fsl/tree/taobao/user_profile.csv\"\n",
        "    user_path = path_manager.open(user, \"rb\")\n",
        "    user_profile_df = pd.read_csv(user_path)\n",
        "\n",
        "    # memory optimize for ad feature dataframe\n",
        "    optimized_gl = raw_sample_df.copy()\n",
        "\n",
        "    gl_int = raw_sample_df.select_dtypes(include=['int'])\n",
        "    converted_int = gl_int.apply(pd.to_numeric,downcast='unsigned')\n",
        "    optimized_gl[converted_int.columns] = converted_int\n",
        "\n",
        "\n",
        "    gl_obj = raw_sample_df.select_dtypes(include=['object']).copy()\n",
        "    converted_obj = pd.DataFrame()\n",
        "    for col in gl_obj.columns:\n",
        "        num_unique_values = len(gl_obj[col].unique())\n",
        "        num_total_values = len(gl_obj[col])\n",
        "        if num_unique_values / num_total_values < 0.5:\n",
        "            converted_obj.loc[:,col] = gl_obj[col].astype('category')\n",
        "        else:\n",
        "            converted_obj.loc[:,col] = gl_obj[col]\n",
        "    optimized_gl[converted_obj.columns] = converted_obj\n",
        "    raw_sample_df = optimized_gl.copy()\n",
        "    raw_sample_df_new = raw_sample_df.rename(columns = {\"user\": \"userid\"})\n",
        "    optimized_g2 = ad_feature_df.copy()\n",
        "    g2_int = ad_feature_df.select_dtypes(include=['int'])\n",
        "    converted_int = g2_int.apply(pd.to_numeric,downcast='unsigned')\n",
        "    optimized_g2[converted_int.columns] = converted_int\n",
        "\n",
        "    g2_float = ad_feature_df.select_dtypes(include=['float'])\n",
        "    converted_float = g2_float.apply(pd.to_numeric,downcast='float')\n",
        "    optimized_g2[converted_float.columns] = converted_float\n",
        "\n",
        "    optimized_g3 = user_profile_df.copy()\n",
        "\n",
        "    g3_int = user_profile_df.select_dtypes(include=['int'])\n",
        "    converted_int = g3_int.apply(pd.to_numeric,downcast='unsigned')\n",
        "    optimized_g3[converted_int.columns] = converted_int\n",
        "\n",
        "    g3_float = user_profile_df.select_dtypes(include=['float'])\n",
        "    converted_float = g3_float.apply(pd.to_numeric,downcast='float')\n",
        "    optimized_g3[converted_float.columns] = converted_float\n",
        "\n",
        "    # combine 3 tables\n",
        "    df1 = raw_sample_df_new.merge(optimized_g3, on=\"userid\")\n",
        "    final_df = df1.merge(optimized_g2, on=\"adgroup_id\")\n",
        "\n",
        "    final_df['pvalue_level'] = final_df['pvalue_level'].fillna(2, )\n",
        "    final_df['final_gender_code'] = final_df['final_gender_code'].fillna(1, )\n",
        "    final_df['age_level'] = final_df['age_level'].fillna(3, )\n",
        "    final_df['shopping_level'] = final_df['shopping_level'].fillna(2, )\n",
        "    final_df['occupation'] = final_df['occupation'].fillna(0, )\n",
        "    final_df['brand'] = final_df['brand'].fillna(0, )\n",
        "    final_df['customer'] = final_df['customer'].fillna(0, )\n",
        "    final_df['cms_group_id'] = final_df['cms_group_id'].fillna(13, )\n",
        "\n",
        "    final_df['pvalue_level'] -= 1\n",
        "    final_df['shopping_level'] -= 1\n",
        "    final_df = final_df.astype({\"cms_segid\": int, \n",
        "                                \"cms_group_id\": int, \n",
        "                                'clk': float, \n",
        "                                'adgroup_id': int, \n",
        "                                'final_gender_code':int,\n",
        "                                'age_level':int,\n",
        "                                'pvalue_level':int,\n",
        "                                'shopping_level':int,\n",
        "                                'occupation':int,\n",
        "                                'cate_id':int,\n",
        "                                'customer':int,\n",
        "                                'brand':int}\n",
        "                                )\n",
        "\n",
        "    return final_df"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "2ccb911a-1fe0-420b-98f4-d55ac6449a02",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "df87b1bf-aa44-4fec-bf7d-c72b3301f97d",
        "executionStartTime": 1677231311296,
        "executionStopTime": 1677231410688
      },
      "source": [
        "tb_df = load_taobao_df()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "2d6b55fb-de9d-4d03-81c3-27bdf55f3645",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "9089579d-4d51-4d38-bb82-9b8a04bee4fc",
        "executionStartTime": 1677231413177,
        "executionStopTime": 1677231413196
      },
      "source": [
        "# checking the dataset\n",
        "# how many label 1 \n",
        "# so that the dataset is very unbalanced\n",
        "sum(tb_df['clk'])/len(tb_df)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.051320095719300095"
          },
          "metadata": {
            "bento_obj_id": "139818259890032"
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "cdfb4483-4970-45c5-a7aa-838aeb6e40ae",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "9298f875-bef4-4659-ad24-4dc09b37f923",
        "executionStartTime": 1677231413605,
        "executionStopTime": 1677231419249
      },
      "source": [
        "# can we get a balanced dataset, by taking the same number of positive label and negative labels\n",
        "# so that training would be ok for now\n",
        "df_1 = tb_df[tb_df['clk'] == 1]\n",
        "df_0 = tb_df[tb_df['clk'] == 0]\n",
        "print(len(df_1))\n",
        "print(len(df_0))\n",
        "frames = [df_1, df_0[:1284513]]\n",
        "df = pd.concat(frames)\n",
        "print(len(df))\n",
        "df = df.sample(frac=1)\n",
        "tb_train = df[:50000]\n",
        "tb_test = df[-10000:]\n",
        "tb_test = tb_test.reset_index()\n",
        "tb_train = tb_train.reset_index()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1284513\n23744922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2569026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "c48bf209-cfe4-4d53-bfcb-d4917b907056",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "5e78326a-02fc-44c7-8b8d-9243edb99965",
        "executionStartTime": 1677231419552,
        "executionStopTime": 1677231419557
      },
      "source": [
        "# train test split\n",
        "#tb_train = tb_df[:2252650] # 90% for training\n",
        "#tb_test = tb_df[2252650:]\n",
        "#tb_test = tb_test.reset_index()\n",
        "\n",
        "# cpu slow, so just test a few \n",
        "#tb_train = tb_df[:50000] # 90% for training\n",
        "#tb_test = tb_df[-10000:]\n",
        "#tb_test = tb_test.reset_index()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "47d39846-16f2-4d3f-a8c0-7b606cbbfede",
        "showInput": false,
        "customInput": null
      },
      "source": [
        "## Model: server_arch and client_arch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "53f0bc51-a5bf-4b9e-b7ca-d76abce97945",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "3da6328b-c423-46ef-a200-ff926f73faf4",
        "executionStartTime": 1677231419853,
        "executionStopTime": 1677231419934
      },
      "source": [
        "class Dataset_split(Dataset):\n",
        "    def __init__(self, X, emb_cols, server_continuous_cols, non_emb_cols):\n",
        "        X = X.copy()\n",
        "        self.server_categorical = X.loc[:,emb_cols].copy().values.astype(np.int64) #categorical columns\n",
        "        self.client = X.reindex(columns = non_emb_cols).values.astype(np.float32)\n",
        "        self.server_continuous = X.reindex(columns = server_continuous_cols).values.astype(np.float32)\n",
        "        self.y = X['clk']\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        #return self.server_categorical[idx], self.server_continuous[idx], self.client[idx], self.y[idx]\n",
        "\n",
        "        return {'server_categorical': self.server_categorical[idx],\n",
        "                'server_continuous': self.server_continuous[idx],\n",
        "                'client': self.client[idx],\n",
        "                'label': self.y[idx]}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "17cdb6e3-8c12-447b-95e0-14fbe4283011",
        "showInput": true,
        "customInput": null
      },
      "source": [
        "Create 2 parts of model: server_arch and client_arch like in FSL project. \n",
        "\n",
        "The server_arch takes in 2 types of input: categorical and continuous. The categorical input will go through the embedding layer first and then concatenate with the continuous input. There is only 1 continuous features 'price' in this dataset.\n",
        "\n",
        "The client_arch takes in only one input. In our case, all 5 features are categorical, which are user related and we are going to reconstructed these 5 features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "15302902-2e64-4ee4-933b-224f5e9ed831",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "1c988536-92df-4f26-9745-6ff52dece7dc",
        "executionStartTime": 1677231420220,
        "executionStopTime": 1677231420295
      },
      "source": [
        "# server side arch\n",
        "class server_arch(nn.Module):\n",
        "    def __init__(self, embedding_sizes, n_cont):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
        "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
        "        self.n_emb, self.n_cont = 250, n_cont\n",
        "        self.emb_drop = nn.Dropout(0.6)\n",
        "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 64)\n",
        "        \n",
        "\n",
        "    def forward(self, x_server_categorical,x_server_continuous):\n",
        "        x = [e(x_server_categorical[:,i]) for i,e in enumerate(self.embeddings)]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = torch.cat([x,x_server_continuous], 1)\n",
        "        x = self.emb_drop(x)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return x\n",
        "\n",
        "class client_arch(nn.Module):\n",
        "    def __init__(self, n_server_output, n_client_input):\n",
        "        super().__init__()\n",
        "        self.n_server_output = n_server_output\n",
        "        self.n_client_input = n_client_input\n",
        "        self.lin1 = nn.Linear(self.n_server_output + self.n_client_input, 32)\n",
        "        self.lin2 = nn.Linear(32, 32)\n",
        "        self.lin3 = nn.Linear(32, 1)\n",
        "        #self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
        "        #self.bn2 = nn.BatchNorm1d(64)\n",
        "        #self.bn3 = nn.BatchNorm1d(32)\n",
        "        #self.emb_drop = nn.Dropout(0.6)\n",
        "        #self.drops = nn.Dropout(0.3)\n",
        "        \n",
        "\n",
        "    def forward(self, server_output, x_client):\n",
        "        cut_layer = torch.cat([server_output, x_client], 1)\n",
        "        x = F.relu(self.lin1(cut_layer))\n",
        "        #x = self.drops(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = F.relu(self.lin2(x))\n",
        "        #x = self.drops(x)\n",
        "        #x = self.bn3(x)\n",
        "        x = torch.sigmoid(self.lin3(x))\n",
        "        return x\n",
        "    \n",
        "    def set_model_gradients(self, gradient):\n",
        "        params = list(self.parameters())\n",
        "        for p, grad in zip(params, gradient):\n",
        "            p.grad = grad.clone()\n",
        "\n",
        "    def backward(self, cut_node, output, target, loss_fn, update_model_grad=True):\n",
        "        loss = loss_fn(output, target)\n",
        "        params = list(self.parameters())\n",
        "                \n",
        "        # gradients dE/dw_n\n",
        "        de_dw = torch.autograd.grad(\n",
        "            [loss],\n",
        "            params,\n",
        "            retain_graph = True,\n",
        "        )\n",
        "\n",
        "        if update_model_grad:\n",
        "            self.set_model_gradients(de_dw)\n",
        "\n",
        "\n",
        "        de_dx = torch.autograd.grad(\n",
        "            [loss],\n",
        "            cut_node,\n",
        "            retain_graph = True,\n",
        "        )\n",
        "\n",
        "        return de_dw, de_dx\n",
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "109185d3-88a8-4b5c-adf4-8189d65c2e9f",
        "showInput": false,
        "customInput": null
      },
      "source": [
        "## fit() and validate() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "aec68e28-e322-43d4-80ba-78e7389d5c0b",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "fc668bb6-0022-4ea0-bda0-dfba4005969a",
        "executionStartTime": 1677231420650,
        "executionStopTime": 1677231420733
      },
      "source": [
        "def validate_split(server_model, client_model, loss_fn, test_dataloader, device):\n",
        "    server_model.eval()\n",
        "    client_model.eval()\n",
        "    total = 0\n",
        "    sum_loss = 0\n",
        "    correct = 0\n",
        "    y_proba_list = []\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "    for batch in test_dataloader:\n",
        "        for key, value in batch.items():\n",
        "                batch[key] = batch[key].to(device)\n",
        "\n",
        "        y = batch['label'].unsqueeze(1).to(torch.float32)\n",
        "\n",
        "        current_batch_size = y.shape[0]\n",
        "\n",
        "        server_output = server_model(batch['server_categorical'], batch['server_continuous'])\n",
        "        out = client_model(server_output, batch['client'])\n",
        "        \n",
        "        y_proba_list.extend(out.detach().cpu().numpy())\n",
        "        y_true_list.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        loss = loss_fn(out, y)\n",
        "        sum_loss += current_batch_size*(loss.item())\n",
        "        total += current_batch_size\n",
        "        pred = torch.round(out).squeeze()\n",
        "\n",
        "        y_pred_list.extend(pred.detach().cpu().numpy())\n",
        "        y = y.squeeze()\n",
        "        correct += (pred == y).sum()\n",
        "\n",
        "        \n",
        "\n",
        "    print(\"test loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
        "    auc = metrics.average_precision_score(y_true_list,y_proba_list)\n",
        "    print(\"AUC\")\n",
        "    print(auc)\n",
        "    conf_matrix = metrics.confusion_matrix(y_true_list,y_pred_list)\n",
        "    print('confusion matrix')\n",
        "    print(conf_matrix)\n",
        "    return sum_loss/total, correct/total"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7222e519-c696-48ea-b792-09efe157272b",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "07351358-a0ea-4dbb-b2c6-45b2fe0463ef",
        "executionStartTime": 1677231421057,
        "executionStopTime": 1677231421124
      },
      "source": [
        "# build training loop that for separate model\n",
        "def one_round_split_train(server_model, client_model, server_opt, client_opt, batch , batch_idx, loss_fn):\n",
        "    # zero all optimizers\n",
        "    server_opt.zero_grad()\n",
        "    client_opt.zero_grad()\n",
        "\n",
        "    server_output = server_model(batch['server_categorical'],batch['server_continuous'])\n",
        "    client_output = client_model(server_output,batch['client'])\n",
        "\n",
        "    label = batch['label'].unsqueeze(1).to(torch.float32)\n",
        "\n",
        "    loss = loss_fn(client_output, label)\n",
        "\n",
        "    # backward\n",
        "    dw, dx = client_model.backward(server_output, client_output, label,loss_fn, update_model_grad=True)\n",
        "\n",
        "    server_output.backward(dx)\n",
        "    server_opt.step()\n",
        "    client_opt.step()\n",
        "    return loss, client_output     "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "09302d67-184d-4877-bff8-ecce9c7031fa",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "54ceb079-5d9e-485e-afc6-c6e20f7e3d95",
        "executionStopTime": 1677231421470,
        "executionStartTime": 1677231421392
      },
      "source": [
        "def fit(server_model, client_model, trainloader, testloader, epochs, lr= 1, momentum=0):\n",
        "    loss_fn = nn.BCELoss()\n",
        "    server_opt = torch.optim.SGD(server_model.parameters(),lr=lr, momentum = momentum)\n",
        "    client_opt = torch.optim.SGD(client_model.parameters(),lr=lr, momentum = momentum)\n",
        "\n",
        "    batch_idx = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    print('starting fiting')\n",
        "    for e in range(0,epochs):\n",
        "        print('epoch', e)\n",
        "        for batch in trainloader:\n",
        "            batch_idx += 1\n",
        "            #print(batch_idx)\n",
        "            for key, value in batch.items():\n",
        "                batch[key] = batch[key].to(device)\n",
        "\n",
        "            loss, output = one_round_split_train(server_model, client_model, server_opt, client_opt, batch, batch_idx, loss_fn)\n",
        "            total_loss += loss\n",
        "            total += batch['label'].shape[0]\n",
        "\n",
        "            if batch_idx % 500 == 0:\n",
        "                val_loss, val_acc = validate_split(server_model, client_model, loss_fn, testloader, device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "5e4b2cbd-b89d-40fe-a46b-dc816ada7aa5",
        "showInput": false,
        "customInput": null
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "4771baf3-e810-48b7-829a-348b285a1ce6",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "48b8d1d6-6483-4e7b-8dd0-5b6fa3e85abe",
        "executionStartTime": 1677231421824,
        "executionStopTime": 1677231421950
      },
      "source": [
        "#creating train and valid datasets\n",
        "client_cols = ['cms_group_id','age_level','pvalue_level','shopping_level','occupation']\n",
        "embedding_cols = ['adgroup_id','cate_id','customer','brand','cms_segid']\n",
        "continuous = ['price']\n",
        "trainset = Dataset_split(tb_train, embedding_cols, continuous, client_cols)\n",
        "testset = Dataset_split(tb_test, embedding_cols, continuous, client_cols)\n",
        "print(len(trainset))\n",
        "print(len(testset))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "f84f7455-8239-42ca-aac7-484e72a293cc",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "699f9429-004c-4585-8d41-d20075ddaf6e",
        "executionStartTime": 1677231422392,
        "executionStopTime": 1677232174550
      },
      "source": [
        "batch_size = 32\n",
        "train_dataloader = DataLoader(trainset, batch_size=batch_size,shuffle=True)\n",
        "test_dataloader = DataLoader(testset, batch_size=batch_size,shuffle=True)\n",
        "\n",
        "# fitting\n",
        "epochs = 2\n",
        "lr = 0.01\n",
        "momentum = 0\n",
        "device = 'cpu'\n",
        "embedded_cols = {'adgroup_id':846811,\n",
        "            'cate_id': 12960,\n",
        "            'customer': 255875,\n",
        "            'brand': 461497,\n",
        "            'cms_segid': 96}\n",
        "embedding_sizes = [(846811+1, 50), (12960+1,50), (255875+1,50),(461497+1,50),(96+1,50)]\n",
        "client_cols = ['cms_group_id','age_level','pvalue_level','shopping_level','occupation']\n",
        "server_continous = ['price']\n",
        "server_model = server_arch(embedding_sizes, 1).to(device)\n",
        "client_model = client_arch(64, len(client_cols)).to(device)\n",
        "\n",
        "fit(server_model, client_model,train_dataloader, test_dataloader, epochs, lr, momentum)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting fiting\nepoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.687 and accuracy 0.536\nAUC\n0.6089683804519889\nconfusion matrix\n[[4276  609]\n [4034 1081]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.680 and accuracy 0.590\nAUC\n0.6254590467321528\nconfusion matrix\n[[3410 1475]\n [2630 2485]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.684 and accuracy 0.535\nAUC\n0.6245446014900116\nconfusion matrix\n[[4233  652]\n [3998 1117]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.680 and accuracy 0.594\nAUC\n0.6359430916442365\nconfusion matrix\n[[3789 1096]\n [2966 2149]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.679 and accuracy 0.577\nAUC\n0.6396847834433075\nconfusion matrix\n[[4068  817]\n [3416 1699]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.675 and accuracy 0.582\nAUC\n0.6460644597865597\nconfusion matrix\n[[3894  991]\n [3185 1930]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "ea915e44-c869-4bea-bb5d-231a3769117f",
        "showInput": false,
        "customInput": null
      },
      "source": [
        "## Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "a1881412-4607-4b4e-8848-12126559c184",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "fb94fc42-cb35-483e-8db3-694a6e5677ed",
        "executionStartTime": 1677232174834,
        "executionStopTime": 1677232174911
      },
      "source": [
        "class Reconstruct_taobao:\n",
        "    def __init__(self, server_model, client_model):\n",
        "        self.server_model = server_model\n",
        "        self.client_model = client_model\n",
        "\n",
        "    def reconstruct(\n",
        "        self,\n",
        "        cluster_combination_list,\n",
        "        server_output,\n",
        "        label,\n",
        "        dx_original,\n",
        "        cate_original,\n",
        "        offsite_feature_shape=5,\n",
        "    ):\n",
        "        self.client_model.eval()\n",
        "        #cluster_combination_list = self.get_cluster_combination(cluster_centers)\n",
        "        dx_diff_list = []\n",
        "\n",
        "        for cate_try in cluster_combination_list:\n",
        "            # forward pass the client side model\n",
        "            # use the other data, but the reconstructed cluster center to be offsite feature to see if dx match.\n",
        "            cate_try = torch.tensor(cate_try).type(torch.float32)\n",
        "            cate_try = cate_try.unsqueeze(0)\n",
        "            \n",
        "            client_output_recon = client_output = client_model(server_output,cate_try)\n",
        "\n",
        "            # backward pass the client side model\n",
        "            # get the gradient and compare with dx and store\n",
        "            \n",
        "            dw_recon, dx_recon = client_model.backward(\n",
        "                            server_output, \n",
        "                            client_output_recon, \n",
        "                            label,\n",
        "                            loss_fn, \n",
        "                            update_model_grad=False)\n",
        "\n",
        "            dx_recon = dx_recon[0]\n",
        "            # calculate the distance between dx and dx_original\n",
        "            # to get the smallest distance as the reconstructed center\n",
        "            dx_diff = self.get_dist_between_dx(dx_recon, dx_original)\n",
        "            dx_diff_list.append(dx_diff.clone().detach())\n",
        "\n",
        "        # output the lowest distance cluster center\n",
        "        minpos = dx_diff_list.index(min(dx_diff_list))\n",
        "        reconstruct_output = cluster_combination_list[minpos]\n",
        "\n",
        "        # check for accuracy\n",
        "        # assert center_original in cluster_combination_list\n",
        "        cate_original = cate_original[0].int()\n",
        "\n",
        "        reconstruct_output = torch.tensor(reconstruct_output)\n",
        "\n",
        "        #Â get the acc for each features\n",
        "        acc = [int(i == j) for i, j in zip(reconstruct_output, cate_original)]\n",
        "        \n",
        "        return acc\n",
        "\n",
        "    def get_dist_between_dx(self, dx_recon, dx_original):\n",
        "        pdist = torch.nn.PairwiseDistance(p=2)\n",
        "        pdistloss = torch.mean(pdist(dx_recon, dx_original))\n",
        "        return pdistloss"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "1fbe39a7-046c-4642-8a8b-d1461ee9c4ba",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "2e3ef2ea-0b66-4d5b-a6da-4013c9a3ae03",
        "executionStartTime": 1677232175192,
        "executionStopTime": 1677232175196
      },
      "source": [
        "# start doing the attack\n",
        "# testloader too to have batch 1\n",
        "def attack_cluster(testloader, cluster_combination_list, server_model, client_model, loss_fn, num_features=5):\n",
        "    stats_output= {}\n",
        "    acc_total = [0 for i in range(num_features)]\n",
        "    num_samples = 0\n",
        "    # initiaze the reconstruct machine\n",
        "    recon_machine = Reconstruct_taobao(\n",
        "        server_model = server_model,\n",
        "        client_model = client_model\n",
        "    )\n",
        "\n",
        "    server_model.eval()\n",
        "    client_model.eval()\n",
        "\n",
        "    for batch_idx, batch in enumerate(testloader):\n",
        "        num_samples += 1\n",
        "        if batch_idx < 4500:\n",
        "            continue\n",
        "        cate_original = batch['client']\n",
        "        \n",
        "        #print('cate original', cate_original.size())\n",
        "        label = batch['label'].unsqueeze(1).to(torch.float32)\n",
        "\n",
        "        server_output_original = server_model(batch['server_categorical'],batch['server_continuous'])\n",
        "        client_output_original = client_model(server_output_original,batch['client'])\n",
        "\n",
        "        dw_original, dx_original = client_model.backward(\n",
        "                server_output_original, \n",
        "                client_output_original, \n",
        "                label,\n",
        "                loss_fn, \n",
        "                update_model_grad=False)\n",
        "        dx_original = dx_original[0]\n",
        "\n",
        "        # doing the attack\n",
        "        acc = recon_machine.reconstruct(\n",
        "                    cluster_combination_list,\n",
        "                    server_output_original,\n",
        "                    label,\n",
        "                    dx_original,\n",
        "                    cate_original,\n",
        "        )\n",
        "        \n",
        "        acc_total = [sum(i) for i in zip(acc_total, acc)]  \n",
        "\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('batch idx', batch_idx)\n",
        "            print('acc_total', acc_total)\n",
        "            print('acc per features', [x / num_samples for x in acc_total])\n",
        "            print('acc average over features', sum(acc_total)/(num_samples*num_features))\n",
        "        \n",
        "    stats_ouput['acc_per_features'] = [x / num_samples for x in acc_total]\n",
        "    stats_output['acc_avg'] = sum(acc_total)/(num_samples*num_features)\n",
        "    print(stats_ouput['acc'] )\n",
        "    return stats_output\n",
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "af552530-6b63-491b-af65-14803771e985",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "41727d4d-a33e-4907-8fe9-4c2272b298a4",
        "executionStartTime": 1677232175617,
        "executionStopTime": 1677232175699
      },
      "source": [
        "test_dataloader_attack = DataLoader(testset, batch_size=1,shuffle=False)\n",
        "loss_fn = nn.BCELoss()\n",
        "# features can be attacked\n",
        "feature_offsite = ['cms_group_id','age_level','pvalue_level','shopping_level','occupation']\n",
        "offsite_category = [13, 7, 3, 3, 2] \n",
        "cluster = []\n",
        "\n",
        "# create a combination\n",
        "for i in range(len(offsite_category)):\n",
        "    temp = [j for j in range(offsite_category[i])]\n",
        "    cluster.append(temp)\n",
        "\n",
        "cluster_combination_list = list(itertools.product(*cluster))\n",
        "print('total number of diffierent combination of category:', len(cluster_combination_list))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of diffierent combination of category: 1638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "6ec6e494-24a5-4be5-9c3a-a954524b9896",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "0e26f34e-5d79-4989-93b4-2678b3deb993",
        "executionStartTime": 1677232176035,
        "executionStopTime": 1677232176037
      },
      "source": [
        "# run the attack\n",
        "output = attack_cluster(test_dataloader_attack, cluster_combination_list, server_model, client_model, loss_fn)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 4500\nacc_total [1, 1, 1, 1, 1]\nacc per features [0.00022217285047767163, 0.00022217285047767163, 0.00022217285047767163, 0.00022217285047767163, 0.00022217285047767163]\nacc average over features 0.00022217285047767163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 5000\nacc_total [454, 464, 466, 470, 478]\nacc per features [0.09078184363127374, 0.09278144371125775, 0.09318136372725455, 0.09398120375924815, 0.09558088382323535]\nacc average over features 0.09326134773045391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 5500\nacc_total [904, 912, 934, 933, 946]\nacc per features [0.1643337574986366, 0.16578803853844756, 0.16978731139792766, 0.16960552626795128, 0.17196873295764406]\nacc average over features 0.16829667333212142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 6000\nacc_total [1362, 1374, 1392, 1396, 1419]\nacc per features [0.22696217297117147, 0.22896183969338443, 0.23196133977670388, 0.23262789535077488, 0.23646058990168306]\nacc average over features 0.23139476753874355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 6500\nacc_total [1813, 1832, 1857, 1865, 1889]\nacc per features [0.2788801722811875, 0.28180279956929705, 0.28564836179049374, 0.2868789417012767, 0.2905706814336256]\nacc average over features 0.2847561913551761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 7000\nacc_total [2261, 2286, 2312, 2331, 2360]\nacc per features [0.32295386373375234, 0.3265247821739751, 0.33023853735180686, 0.33295243536637625, 0.3370947007570347]\nacc average over features 0.32995286387658906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 7500\nacc_total [2709, 2747, 2772, 2793, 2835]\nacc per features [0.36115184642047726, 0.36621783762165044, 0.3695507265697907, 0.3723503532862285, 0.3779496067191041]\nacc average over features 0.3694440741234502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 8000\nacc_total [3154, 3202, 3237, 3246, 3300]\nacc per features [0.39420072490938635, 0.4001999750031246, 0.40457442819647543, 0.4056992875890514, 0.4124484439445069]\nacc average over features 0.40342457192850895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 8500\nacc_total [3608, 3662, 3699, 3712, 3773]\nacc per features [0.4244206563933655, 0.4307728502529114, 0.4351252793788966, 0.4366545112339725, 0.443830137630867]\nacc average over features 0.4341606869780026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 9000\nacc_total [4063, 4121, 4163, 4174, 4252]\nacc per features [0.4513942895233863, 0.4578380179980002, 0.4625041662037551, 0.46372625263859574, 0.4723919564492834]\nacc average over features 0.4615709365626042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch idx 9500\nacc_total [4520, 4589, 4629, 4642, 4729]\nacc per features [0.4757393958530681, 0.4830017892853384, 0.4872118724344806, 0.4885801494579518, 0.49773708030733604]\nacc average over features 0.486454057467635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "8ef77c72-e6a6-451a-9972-740175a5a0f2",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "bff02c04-365c-4943-9eb8-ed64a04c3080",
        "executionStartTime": 1677248662906,
        "executionStopTime": 1677248663007
      },
      "source": [
        "a = [4501, 4499, 4495, 4496, 4500]\n",
        "b = [4520, 4589, 4629, 4642, 4729]\n",
        "c = [(a+b)/9500 for a, b in zip(a,b)]\n",
        "c"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0.9495789473684211,\n 0.9566315789473684,\n 0.960421052631579,\n 0.9618947368421052,\n 0.9714736842105263]"
          },
          "metadata": {
            "bento_obj_id": "140083948930624"
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "3400e657-2023-41e0-986c-06feadd19990",
        "showInput": true,
        "customInput": null
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
